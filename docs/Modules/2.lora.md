# LoRA

LoRA是[LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685) 论文提供的轻量级训练组件。LoRA可以添加到Linear、Embedding、Conv2d等算子上生效。

>```python
>LoRAConfig (
>    r: int LoRA结构的秩
>    target_modules: Union[List[str], str] MLP结构的module_key，如果是str类型则进行full_match统配查找，如果是List，则进行末尾匹配
>    lora_alpha: int LoRA结构的权重比例，lora_alpha/r的值是lora结构的权重
>    lora_dropout: float LoRA结构的dropout比例
>    merge_weights: bool 在推理时是否将loRA权重合并到原始weights上
>    use_merged_linear: bool 是否是merged linear结构
>    enable_lora: List[bool]: 如果是use_merged_linear，哪些module需要添加LoRA结构
>    bias: str 偏置是否参与训练和存储，可以为`none`：所有偏置不参与训练, `all`：所有模块的偏置均参与训练, `lora_only`：仅loRA结构的偏置参与训练
>)
>```

一个使用LoRA的例子如下：

```python
from modelscope import Model
from swift import Swift, LoRAConfig
import torch
model = Model.from_pretrained('ZhipuAI/chatglm2-6b', torch_dtype=torch.bfloat16, device_map='auto')
lora_config = LoRAConfig(
                r=16,
                target_modules=['query_key_value'],
                lora_alpha=32,
                lora_dropout=0.)
model = Swift.prepare_model(model, lora_config)
# use model to do other things
```
